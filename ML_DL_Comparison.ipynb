{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================\n",
    "# Setup\n",
    "# ============================================\n",
    "\n",
    "np.random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "threshold = 0.43\n",
    "class_weight = {0: 1, 1: 1}\n",
    "\n",
    "# Dummy placeholder for top_features_dict â€” replace with actual top 10 features\n",
    "top_features_dict = {\n",
    "    \"Scenario 1\": [...],  # Replace with top 10 features\n",
    "    \"Scenario 2\": [...],\n",
    "    \"Scenario 3\": [...],\n",
    "    \"Scenario 4\": [...]\n",
    "}\n",
    "\n",
    "# Define file paths for each scenario\n",
    "scenarios = {\n",
    "    \"Scenario 1\": {\n",
    "        \"train\": \"df_metabol.xlsx\",\n",
    "        \"validation\": \"combat_validation_no_scaling2.xlsx\",\n",
    "        \"TASOAC\": \"combat_data_no_scaling2.xlsx\",\n",
    "        \"title\": \"Metabolomic features only\"\n",
    "    },\n",
    "    \"Scenario 2\": {\n",
    "        \"train\": \"df_ratio2.xlsx\",\n",
    "        \"validation\": \"combat_validation_no_scaling2_ratio.xlsx\",\n",
    "        \"TASOAC\": \"combat_data_no_scaling3_ratio.xlsx\",\n",
    "        \"title\": \"Metabolomic features + ratios\"\n",
    "    },\n",
    "    \"Scenario 3\": {\n",
    "        \"train\": \"df_inverse_ratios2.xlsx\",\n",
    "        \"validation\": \"combat_validation_no_scaling2_inverse_ratio.xlsx\",\n",
    "        \"TASOAC\": \"combat_data_no_scaling3_inverse_ratio.xlsx\",\n",
    "        \"title\": \"Metabolomic features + inverse ratios\"\n",
    "    },\n",
    "    \"Scenario 4\": {\n",
    "        \"train\": \"df_allratios2.xlsx\",\n",
    "        \"validation\": \"combat_validation_no_scaling2_allratio.xlsx\",\n",
    "        \"TASOAC\": \"combat_data_no_scaling3_all_ratios.xlsx\",\n",
    "        \"title\": \"Metabolomic features + all ratios\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ML/DL models\n",
    "models = {\n",
    "    \"ANN\": {\n",
    "        \"type\": \"ann\",\n",
    "        \"params\": {\n",
    "            \"units\": [8, 16, 32],                \n",
    "            \"dropout\": [0.0, 0.2, 0.5],          \n",
    "            \"batch_size\": [8, 16, 32],           \n",
    "            \"epochs\": [20, 30, 50],              \n",
    "            \"learning_rate\": [0.001, 0.0005, 0.01]  \n",
    "        }\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"type\": \"ml\",\n",
    "        \"model\": LogisticRegression(random_state=123, class_weight=class_weight, max_iter=1000),\n",
    "        \"param_grid\": [\n",
    "            {\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'C': [0.01, 0.1, 1, 10, 100],\n",
    "                'solver': ['saga']\n",
    "            },\n",
    "            {\n",
    "                'penalty': ['elasticnet'],\n",
    "                'C': [0.01, 0.1, 1, 10, 100],\n",
    "                'solver': ['saga'],\n",
    "                'l1_ratio': [0, 0.3, 0.7, 1]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"type\": \"ml\",\n",
    "        \"model\": SVC(probability=True, class_weight=class_weight),\n",
    "        \"param_grid\": {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"type\": \"ml\",\n",
    "        \"model\": RandomForestClassifier(class_weight='balanced'),\n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100],\n",
    "            'max_depth': [None, 10],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [1]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"type\": \"ml\",\n",
    "        \"model\": XGBClassifier(eval_metric='logloss', scale_pos_weight=3),\n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100],\n",
    "            'max_depth': [3, 5],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'subsample': [0.8],\n",
    "            'colsample_bytree': [0.8]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "def calculate_sen_spe(y_true, y_prob, threshold=0.43):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# ============================================\n",
    "# Loop over scenarios\n",
    "# ============================================\n",
    "\n",
    "results = []\n",
    "\n",
    "for scen_key, scen_info in scenarios.items():\n",
    "    print(f\"\\nProcessing {scen_key}...\")\n",
    "\n",
    "    # Load and clean\n",
    "    train = pd.read_excel(scen_info[\"train\"]).rename(columns=lambda x: x.replace('/', '__'))\n",
    "    val = pd.read_excel(scen_info[\"validation\"]).rename(columns=lambda x: x.replace('/', '__'))\n",
    "\n",
    "    # Target assignment\n",
    "    train['Progressor'] = np.where(train['p1'] > threshold, 1, 0)\n",
    "    val['Progressor'] = np.where(val['p1'] > threshold, 1, 0)\n",
    "    train.drop(columns='p1', inplace=True)\n",
    "    val.drop(columns='p1', inplace=True)\n",
    "\n",
    "    # Feature selection\n",
    "    features = top_features_dict[scen_key] + ['Progressor']\n",
    "    train = train[features]\n",
    "    val = val[features]\n",
    "\n",
    "    X = train.drop(columns='Progressor')\n",
    "    y = train['Progressor']\n",
    "    X_val = val.drop(columns='Progressor')\n",
    "    y_val = val['Progressor']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # SMOTE\n",
    "    sm = SMOTE(random_state=42, sampling_strategy={0: 1500, 1: 1500})\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    for model_name, model_def in models.items():\n",
    "        print(f\" Training {model_name}...\")\n",
    "\n",
    "        # Metrics init\n",
    "        test_auc = val_auc = test_acc = val_acc = test_sen = test_spe = val_sen = val_spe = np.nan\n",
    "\n",
    "        try:\n",
    "            if model_def[\"type\"] == \"ann\":\n",
    "                p = model_def[\"params\"]\n",
    "                model = Sequential([\n",
    "                    Input(shape=(X_train_res.shape[1],)),\n",
    "                    Dense(p['units'], activation='relu'),\n",
    "                    Dropout(p['dropout']),\n",
    "                    Dense(1, activation='sigmoid')\n",
    "                ])\n",
    "                model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=p['learning_rate']),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy']\n",
    "                )\n",
    "                model.fit(X_train_res, y_train_res,\n",
    "                          epochs=p['epochs'], batch_size=p['batch_size'], verbose=0)\n",
    "\n",
    "                y_test_prob = model.predict(X_test_scaled).flatten()\n",
    "                y_val_prob = model.predict(X_val_scaled).flatten()\n",
    "\n",
    "            else:\n",
    "                grid = GridSearchCV(model_def[\"model\"], model_def[\"param_grid\"], scoring='roc_auc', cv=3)\n",
    "                grid.fit(X_train_res, y_train_res)\n",
    "                best_model = grid.best_estimator_\n",
    "                y_test_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "                y_val_prob = best_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "            # Test set metrics\n",
    "            y_test_pred = (y_test_prob >= threshold).astype(int)\n",
    "            test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "            test_acc = accuracy_score(y_test, y_test_pred)\n",
    "            test_sen, test_spe = calculate_sen_spe(y_test, y_test_prob)\n",
    "\n",
    "            # Validation set metrics\n",
    "            y_val_pred = (y_val_prob >= threshold).astype(int)\n",
    "            val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "            val_acc = accuracy_score(y_val, y_val_pred)\n",
    "            val_sen, val_spe = calculate_sen_spe(y_val, y_val_prob)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {model_name}: {e}\")\n",
    "\n",
    "        for dataset, auc, acc, sen, spe in [\n",
    "            (\"Test\", test_auc, test_acc, test_sen, test_spe),\n",
    "            (\"Validation\", val_auc, val_acc, val_sen, val_spe)\n",
    "        ]:\n",
    "            results.append({\n",
    "                \"Scenario\": scen_key,\n",
    "                \"Model\": model_name,\n",
    "                \"Dataset\": dataset,\n",
    "                \"AUC\": auc,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Sensitivity\": sen,\n",
    "                \"Specificity\": spe\n",
    "            })\n",
    "\n",
    "# ============================================\n",
    "# Save & Plot Results\n",
    "# ============================================\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(\"model_results.xlsx\", index=False)\n",
    "print(\"\\nFinal Results:\\n\", results_df.round(3).to_markdown(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
